"""
ç´¢å¼•æœåŠ¡
ç®¡ç†å‘é‡ç´¢å¼•çš„åˆ›å»ºã€æ›´æ–°å’ŒæŒä¹…åŒ–
ä½¿ç”¨ LlamaIndex å†…ç½®çš„ç®€å•å‘é‡å­˜å‚¨ (æ— éœ€ chromadb)
"""
import hashlib
import os
import json
from pathlib import Path
from typing import List, Dict, Optional
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    Settings,
    load_index_from_storage
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.dashscope import DashScope
from llama_index.embeddings.dashscope import DashScopeEmbedding

from app.config import settings
from app.services.document_loader import document_loader, Document


class IndexService:
    """ç´¢å¼•ç®¡ç†æœåŠ¡"""
    
    PERSIST_DIR = None
    
    def __init__(self):
        """åˆå§‹åŒ–ç´¢å¼•æœåŠ¡"""
        self._index: Optional[VectorStoreIndex] = None
        self._file_hashes: Dict[str, str] = {}
        
        # è®¾ç½®æŒä¹…åŒ–ç›®å½•
        self.PERSIST_DIR = str(settings.chroma_path / "index")
        
        # é…ç½® LlamaIndex å…¨å±€è®¾ç½®
        self._configure_llm_settings()
    
    def _configure_llm_settings(self):
        """é…ç½® LLM å’Œ Embedding æ¨¡å‹"""
        if settings.dashscope_api_key:
            # è®¾ç½®ç¯å¢ƒå˜é‡ (dashscope éœ€è¦)
            os.environ["DASHSCOPE_API_KEY"] = settings.dashscope_api_key
            
            Settings.llm = DashScope(
                model_name=settings.llm_model,
                api_key=settings.dashscope_api_key,
                max_tokens=2048  # é€‚å½“çš„è¾“å‡º token é™åˆ¶
            )
            Settings.embed_model = DashScopeEmbedding(
                model_name=settings.embedding_model,
                api_key=settings.dashscope_api_key
            )
        
        # é…ç½®æ–‡æœ¬åˆ†å—
        Settings.node_parser = SentenceSplitter(
            chunk_size=512,
            chunk_overlap=50
        )
    
    def get_or_create_index(self) -> VectorStoreIndex:
        """
        è·å–æˆ–åˆ›å»ºå‘é‡ç´¢å¼•
        
        Returns:
            VectorStoreIndex å®ä¾‹
        """
        if self._index is not None:
            return self._index
        
        persist_path = Path(self.PERSIST_DIR)
        
        # å°è¯•ä»å­˜å‚¨åŠ è½½
        if persist_path.exists() and (persist_path / "docstore.json").exists():
            try:
                storage_context = StorageContext.from_defaults(persist_dir=self.PERSIST_DIR)
                self._index = load_index_from_storage(storage_context)
                print(f"âœ… Loaded existing index from {self.PERSIST_DIR}")
                return self._index
            except Exception as e:
                print(f"âš ï¸ Failed to load index: {e}, will create new one")
        
        # åˆ›å»ºæ–°çš„ç©ºç´¢å¼•
        self._index = VectorStoreIndex([])
        return self._index
    
    def build_full_index(self) -> int:
        """
        æ„å»ºå®Œæ•´ç´¢å¼•ï¼ˆé¦–æ¬¡æˆ–é‡å»ºï¼‰
        
        Returns:
            ç´¢å¼•çš„æ–‡æ¡£æ•°é‡
        """
        # åŠ è½½æ‰€æœ‰æ–‡æ¡£
        documents = document_loader.load_all_documents()
        
        if not documents:
            print("âš ï¸ No documents found to index")
            return 0
        
        print(f"ğŸ“š Found {len(documents)} documents to index")
        
        # åˆ›å»ºæ–°ç´¢å¼•
        self._index = VectorStoreIndex.from_documents(
            documents=documents,
            show_progress=True
        )
        
        # æŒä¹…åŒ–
        persist_path = Path(self.PERSIST_DIR)
        persist_path.mkdir(parents=True, exist_ok=True)
        self._index.storage_context.persist(persist_dir=self.PERSIST_DIR)
        
        print(f"âœ… Index built and saved to {self.PERSIST_DIR}")
        
        # è®°å½•æ–‡ä»¶å“ˆå¸Œ
        for doc in documents:
            file_path = doc.metadata.get("file_path")
            if file_path:
                self._file_hashes[file_path] = self._compute_file_hash(file_path)
        
        return len(documents)
    
    def update_document(self, file_path: str) -> bool:
        """
        æ›´æ–°å•ä¸ªæ–‡æ¡£ï¼ˆå¢é‡æ›´æ–°ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        index = self.get_or_create_index()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸçš„å˜æ›´äº†
        current_hash = self._compute_file_hash(file_path)
        if file_path in self._file_hashes and self._file_hashes[file_path] == current_hash:
            return False  # æ–‡ä»¶æœªå˜æ›´
        
        # åŠ è½½æ–°æ–‡æ¡£
        document = document_loader.load_single_document(file_path)
        if document is None:
            return False
        
        # åˆ é™¤æ—§çš„ç´¢å¼•æ¡ç›®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            index.delete_ref_doc(file_path, delete_from_docstore=True)
        except Exception:
            pass  # å¯èƒ½ä¸å­˜åœ¨
        
        # æ’å…¥æ–°æ–‡æ¡£
        index.insert(document)
        
        # æŒä¹…åŒ–
        index.storage_context.persist(persist_dir=self.PERSIST_DIR)
        
        # æ›´æ–°å“ˆå¸Œ
        self._file_hashes[file_path] = current_hash
        
        return True
    
    def delete_document(self, file_path: str) -> bool:
        """
        åˆ é™¤æ–‡æ¡£ç´¢å¼•
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        index = self.get_or_create_index()
        
        try:
            index.delete_ref_doc(file_path, delete_from_docstore=True)
            index.storage_context.persist(persist_dir=self.PERSIST_DIR)
            self._file_hashes.pop(file_path, None)
            return True
        except Exception as e:
            print(f"Failed to delete document {file_path}: {e}")
            return False
    
    def _compute_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹å“ˆå¸Œ"""
        try:
            with open(file_path, "rb") as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""
    
    def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        try:
            index = self.get_or_create_index()
            # è·å–æ–‡æ¡£æ•°é‡
            doc_count = len(index.docstore.docs) if hasattr(index, 'docstore') else 0
            return {
                "total_documents": doc_count,
                "indexed_files": len(self._file_hashes)
            }
        except Exception:
            return {
                "total_documents": 0,
                "indexed_files": 0
            }


# å…¨å±€å®ä¾‹
index_service = IndexService()
